{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJquPiMJycaG"
   },
   "source": [
    "# **Section 1: Dataset and Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CRmsAqrVQnD"
   },
   "source": [
    "## **Step 1: Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxFEROjxVXII"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the datasets\n",
    "with open('/content/sample_data/train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "with open('/content/sample_data/val.json') as f:\n",
    "    valid_data = json.load(f)\n",
    "valid_df = pd.DataFrame(valid_data)\n",
    "\n",
    "with open('/content/sample_data/test.json') as f:\n",
    "    test_data = json.load(f)\n",
    "test_df = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0AR33veVd0R"
   },
   "source": [
    "## **Step 2: Tokenization Function Using spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLGymTQcVhWF"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "nlp.remove_pipe('lemmatizer')\n",
    "\n",
    "\n",
    "def text_pipeline_spacy_special(text):\n",
    "  tokens = []\n",
    "  doc = nlp(text)\n",
    "  for t in doc:\n",
    "    if not t.is_punct and not t.is_space:\n",
    "      tokens.append(t.text.lower())\n",
    "  return tokens\n",
    "\n",
    "\n",
    "# Tokenizing questions and options\n",
    "train_df['Question_Tokens'] = train_df['question'].apply(text_pipeline_spacy_special)\n",
    "train_df['Options_Tokens'] = train_df['options'].apply(lambda opts: [text_pipeline_spacy_special(opt) for opt in opts])\n",
    "valid_df['Question_Tokens'] = valid_df['question'].apply(text_pipeline_spacy_special)\n",
    "valid_df['Options_Tokens'] = valid_df['options'].apply(lambda opts: [text_pipeline_spacy_special(opt) for opt in opts])\n",
    "test_df['Question_Tokens'] = test_df['question'].apply(text_pipeline_spacy_special)\n",
    "test_df['Options_Tokens'] = test_df['options'].apply(lambda opts: [text_pipeline_spacy_special(opt) for opt in opts])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkVmK4m7VkpS"
   },
   "source": [
    "## **Step 3: Pre-process and Analyze the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDeUE1XiVuZ2",
    "outputId": "c0cf2423-2ce1-4f0a-9f68-3ffab13908f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 741 questions.\n",
      "Validation set has 103 questions.\n",
      "Test set has 202 questions.\n",
      "Training set has 2964 options\n",
      "Validation set has 412 options.\n",
      "Test set has 808 options.\n",
      "Average number of tokens per question in the training set: 6.272604588394062\n",
      "Average number of tokens per option in the training set: 22.338056680161944\n",
      "Average number of tokens per correct option in the training set: 26.032388663967613\n",
      "Average lexical overlap with question in correct options: 2.6531713900134952\n",
      "Average lexical overlap with question in incorrect options: 1.605038236617184\n",
      "Average length of correct options: 26.032388663967613\n",
      "Average length of incorrect options: 21.106612685560055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-de170a98507a>:85: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = question.similarity(correct_option)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average semantic similarity between questions and correct options: 0.3151678621389988\n"
     ]
    }
   ],
   "source": [
    "#Q1.1: Count questions and options in each split\n",
    "train_options_count = sum(len(item[\"options\"]) for item in train_data)\n",
    "valid_options_count = sum(len(item[\"options\"]) for item in valid_data)\n",
    "test_options_count = sum(len(item[\"options\"]) for item in test_data)\n",
    "print(f\"Training set has {len(train_df)} questions.\")\n",
    "print(f\"Validation set has {len(valid_df)} questions.\")\n",
    "print(f\"Test set has {len(test_df)} questions.\")\n",
    "print(f\"Training set has {train_options_count} options\")\n",
    "print(f\"Validation set has {valid_options_count} options.\")\n",
    "print(f\"Test set has {test_options_count} options.\")\n",
    "#Q1.2 & Q1.3: Average number of tokens per question and per choice in the training set\n",
    "avg_tokens_question_train = train_df['Question_Tokens'].apply(len).mean()\n",
    "print(f\"Average number of tokens per question in the training set: {avg_tokens_question_train}\")\n",
    "\n",
    "\n",
    "options_lengths_train = [len(token) for sublist in train_df['Options_Tokens'].tolist() for token in sublist]\n",
    "avg_tokens_option_train = sum(options_lengths_train) / len(options_lengths_train)\n",
    "print(f\"Average number of tokens per option in the training set: {avg_tokens_option_train}\")\n",
    "\n",
    "\n",
    "#Q1.4: Average number of tokens per correct choice in the training set\n",
    "def correct_option_tokens(row):\n",
    "    return len(text_pipeline_spacy_special(row['options'][row['correct_index']]))\n",
    "\n",
    "avg_tokens_correct_option_train = train_df.apply(correct_option_tokens, axis=1).mean()\n",
    "print(f\"Average number of tokens per correct option in the training set: {avg_tokens_correct_option_train}\")\n",
    "\n",
    "\n",
    "# Perform any additional exploration of the data that you feel would be helpful for this multiple-choicequestion-answering task. Briefly describe what you found.\n",
    "def lexical_overlap(data):\n",
    "    overlap_correct = []\n",
    "    overlap_incorrect = []\n",
    "\n",
    "    for item in data:\n",
    "        question_tokens = set(text_pipeline_spacy_special(item['question']))\n",
    "\n",
    "        for idx, option in enumerate(item['options']):\n",
    "            option_tokens = set(text_pipeline_spacy_special(option))\n",
    "            shared_tokens = len(question_tokens.intersection(option_tokens))\n",
    "\n",
    "            if idx == item['correct_index']:\n",
    "                overlap_correct.append(shared_tokens)\n",
    "            else:\n",
    "                overlap_incorrect.append(shared_tokens)\n",
    "\n",
    "    avg_overlap_correct = sum(overlap_correct) / len(overlap_correct)\n",
    "    avg_overlap_incorrect = sum(overlap_incorrect) / len(overlap_incorrect)\n",
    "\n",
    "    return avg_overlap_correct, avg_overlap_incorrect\n",
    "\n",
    "avg_overlap_correct_train, avg_overlap_incorrect_train = lexical_overlap(train_data)\n",
    "print(\"Average lexical overlap with question in correct options:\", avg_overlap_correct_train)\n",
    "print(\"Average lexical overlap with question in incorrect options:\", avg_overlap_incorrect_train)\n",
    "\n",
    "def option_length_comparison(data):\n",
    "    lengths_correct = []\n",
    "    lengths_incorrect = []\n",
    "\n",
    "    for item in data:\n",
    "        for idx, option in enumerate(item['options']):\n",
    "            option_length = len(text_pipeline_spacy_special(option))\n",
    "\n",
    "            if idx == item['correct_index']:\n",
    "                lengths_correct.append(option_length)\n",
    "            else:\n",
    "                lengths_incorrect.append(option_length)\n",
    "\n",
    "    avg_length_correct = sum(lengths_correct) / len(lengths_correct)\n",
    "    avg_length_incorrect = sum(lengths_incorrect) / len(lengths_incorrect)\n",
    "\n",
    "    return avg_length_correct, avg_length_incorrect\n",
    "\n",
    "avg_length_correct_train, avg_length_incorrect_train = option_length_comparison(train_data)\n",
    "print(\"Average length of correct options:\", avg_length_correct_train)\n",
    "print(\"Average length of incorrect options:\", avg_length_incorrect_train)\n",
    "\n",
    "#Semantic Similarity: It measures how much two pieces of text are related in meaning, not just in shared tokens. spaCy's language models can calculate this based on word embeddings, which capture semantic meanings of words.\n",
    "def calculate_semantic_similarity(data):\n",
    "    similarities = []\n",
    "\n",
    "    for item in data:\n",
    "        question = nlp(item['question'])\n",
    "        correct_option = nlp(item['options'][item['correct_index']])\n",
    "        similarity = question.similarity(correct_option)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    avg_similarity = sum(similarities) / len(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "avg_similarity_train = calculate_semantic_similarity(train_data)\n",
    "print(\"Average semantic similarity between questions and correct options:\", avg_similarity_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-y1NH2oBaeR"
   },
   "source": [
    "Expected Insights\n",
    "Similarity Between Question and Options: If the average lexical overlap is higher for correct options, this might indicate that correct answers share more vocabulary with the question, potentially guiding the development of features for machine learning models or rules for rule-based approaches.\n",
    "\n",
    "Option Length Comparison: Should correct options consistently be longer or shorter, this characteristic could serve help in future as we can clearly see that correct options have more average length then the incoreect options.\n",
    "\n",
    "Sementic Similarity : A higher average similarity score between questions and correct options might suggest that leveraging semantic similarity could improve answer selection, but in our case it is not helpful as the socre is really low 0.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RJE_EnMyqTb"
   },
   "source": [
    "# **Section 2: Set Similarity Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd3NiXCUPIs2"
   },
   "source": [
    "## **Step 1: Define the Similarity Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0fVspy3PK89"
   },
   "outputs": [],
   "source": [
    "def overlap_coefficient(set1, set2):\n",
    "    return len(set1.intersection(set2)) / min(len(set1), len(set2))\n",
    "\n",
    "def sorensen_dice_coefficient(set1, set2):\n",
    "    return 2 * len(set1.intersection(set2)) / (len(set1) + len(set2))\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE_f8SFsPNqR"
   },
   "source": [
    "## **Step 2: Calculate Similarities for Each Question-Option Pair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRk4ZjBTPQYj",
    "outputId": "09a3c25e-cea4-4cdd-d5cf-2acac37bb33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure: overlap_coefficient\n",
      "Training Accuracy: 0.5236167341430499, Ties: 246\n",
      "Validation Accuracy: 0.46601941747572817, Ties: 29\n",
      "--------------------------------------------------\n",
      "Measure: sorensen_dice_coefficient\n",
      "Training Accuracy: 0.4291497975708502, Ties: 20\n",
      "Validation Accuracy: 0.3592233009708738, Ties: 4\n",
      "--------------------------------------------------\n",
      "Measure: jaccard_similarity\n",
      "Training Accuracy: 0.4291497975708502, Ties: 20\n",
      "Validation Accuracy: 0.3592233009708738, Ties: 4\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy_and_ties(data, similarity_function):\n",
    "    correct_predictions = 0\n",
    "    tie_counts = 0\n",
    "\n",
    "    for item in data:\n",
    "        question_tokens = set(text_pipeline_spacy_special(item['question']))\n",
    "        scores = []\n",
    "\n",
    "        for option in item['options']:\n",
    "            option_tokens = set(text_pipeline_spacy_special(option))\n",
    "            score = similarity_function(question_tokens, option_tokens)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Check for ties\n",
    "        max_score = max(scores)\n",
    "        if scores.count(max_score) > 1:\n",
    "            tie_counts += 1\n",
    "            # If tied, arbitrarily pick the first occurrence of the max score\n",
    "            predicted_index = scores.index(max_score)\n",
    "        else:\n",
    "            predicted_index = scores.index(max_score)\n",
    "\n",
    "        if predicted_index == item['correct_index']:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(data)\n",
    "    return accuracy, tie_counts\n",
    "\n",
    "for measure in [overlap_coefficient, sorensen_dice_coefficient, jaccard_similarity]:\n",
    "    train_accuracy, train_ties = calculate_accuracy_and_ties(train_data, measure)\n",
    "    valid_accuracy, valid_ties = calculate_accuracy_and_ties(valid_data, measure)\n",
    "    print(f\"Measure: {measure.__name__}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}, Ties: {train_ties}\")\n",
    "    print(f\"Validation Accuracy: {valid_accuracy}, Ties: {valid_ties}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl_ScWbmR3-e"
   },
   "source": [
    "In the provided calculate_accuracy_and_ties function, ties are resolved by selecting the first option with the highest score. This approach is arbitrary but ensures consistent and reproducible behavior. Depending on your specific needs or insights into the dataset, you might choose a different strategy to break ties, such as randomly selecting among tied options or using additional heuristics to make an informed choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M3A6d0Gyu6e"
   },
   "source": [
    "# **Section 3: Cosine similarity of TF vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZlJa6KqVuhR"
   },
   "source": [
    "## **Step 1: Create TF Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRxAvXLQVxim"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text_pipeline_spacy_special(text)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, analyzer='word')\n",
    "\n",
    "def generate_tf_vectors(dataframe):\n",
    "    questions = dataframe['question'].tolist()\n",
    "    options_list = dataframe['options'].tolist()\n",
    "\n",
    "    # Flatten the list\n",
    "    all_texts = questions + [option for options in options_list for option in options]\n",
    "    tf_matrix = vectorizer.fit_transform(all_texts).toarray()\n",
    "\n",
    "    # Split the TF matrix\n",
    "    q_tf_matrix = tf_matrix[:len(questions)]\n",
    "    o_tf_matrix = tf_matrix[len(questions):]\n",
    "\n",
    "    return q_tf_matrix, np.split(o_tf_matrix, len(questions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIIkdZEQaPJK"
   },
   "source": [
    "## **Step 2: Calculating Cosine Similarity and Selecting Best Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3SJD_HyaU6W"
   },
   "outputs": [],
   "source": [
    "def select_best_answer(q_tf_matrix, o_tf_matrix_list):\n",
    "    selected_answers = []\n",
    "    for q_vector, o_matrix in zip(q_tf_matrix, o_tf_matrix_list):\n",
    "        cos_sim = cosine_similarity([q_vector], o_matrix)[0]\n",
    "        selected_answers.append(np.argmax(cos_sim))\n",
    "    return selected_answers\n",
    "\n",
    "def calculate_accuracy(selected_answers, correct_answers):\n",
    "    correct_predictions = sum(1 for selected, correct in zip(selected_answers, correct_answers) if selected == correct)\n",
    "    return correct_predictions / len(correct_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2nino8D2S0P"
   },
   "source": [
    "## **Step 3: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BG0cgrtX2V4f",
    "outputId": "db56cdd0-5634-43dd-9b2b-69a0fe42c148"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.446693657219973\n",
      "Validation Accuracy: 0.4563106796116505\n"
     ]
    }
   ],
   "source": [
    "# Generate TF vectors for questions and options\n",
    "q_tf_matrix_train, o_tf_matrix_list_train = generate_tf_vectors(train_df)\n",
    "q_tf_matrix_valid, o_tf_matrix_list_valid = generate_tf_vectors(valid_df)\n",
    "\n",
    "# Select the best answer based on cosine similarity\n",
    "selected_answers_train = select_best_answer(q_tf_matrix_train, o_tf_matrix_list_train)\n",
    "selected_answers_valid = select_best_answer(q_tf_matrix_valid, o_tf_matrix_list_valid)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train = calculate_accuracy(selected_answers_train, train_df['correct_index'].tolist())\n",
    "accuracy_valid = calculate_accuracy(selected_answers_valid, valid_df['correct_index'].tolist())\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy_train}\")\n",
    "print(f\"Validation Accuracy: {accuracy_valid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nCxZZx_z3xm"
   },
   "source": [
    "## **Modifying the CountVectorizer for Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8Jla0g5z5ZR",
    "outputId": "0df0d38f-177e-4d37-e984-40f35bf6c36e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy with Bigrams: 0.4534412955465587\n",
      "Validation Accuracy with Bigrams: 0.44660194174757284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text_pipeline_spacy_special(text)\n",
    "\n",
    "# Initialize CountVectorizer with both unigrams and bigrams\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, analyzer='word', ngram_range=(1, 2))\n",
    "\n",
    "def generate_tf_vectors_with_bigrams(dataframe):\n",
    "    questions = dataframe['question'].tolist()\n",
    "    options_list = dataframe['options'].tolist()\n",
    "\n",
    "    # Flatten the list\n",
    "    all_texts = questions + [option for options in options_list for option in options]\n",
    "    tf_matrix = vectorizer.fit_transform(all_texts).toarray()\n",
    "\n",
    "    # Split the TF matrix back\n",
    "    q_tf_matrix = tf_matrix[:len(questions)]\n",
    "    o_tf_matrix = tf_matrix[len(questions):]\n",
    "\n",
    "    return q_tf_matrix, np.split(o_tf_matrix, len(questions))\n",
    "\n",
    "def select_best_answer(q_tf_matrix, o_tf_matrix_list):\n",
    "    selected_answers = []\n",
    "    for q_vector, o_matrix in zip(q_tf_matrix, o_tf_matrix_list):\n",
    "        # Reshape the question vector to a 2D array\n",
    "        q_vector_reshaped = q_vector.reshape(1, -1)\n",
    "\n",
    "        # Ensure options matrix is a 2D array;\n",
    "        o_matrix_2d = np.array(o_matrix)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = cosine_similarity(q_vector_reshaped, o_matrix_2d)[0]\n",
    "\n",
    "        # option with the highest similarity\n",
    "        selected_answers.append(np.argmax(cos_sim))\n",
    "    return selected_answers\n",
    "\n",
    "\n",
    "q_tf_matrix_train, o_tf_matrix_list_train = generate_tf_vectors_with_bigrams(train_df)\n",
    "q_tf_matrix_valid, o_tf_matrix_list_valid = generate_tf_vectors_with_bigrams(valid_df)\n",
    "\n",
    "selected_answers_train_bigrams = select_best_answer(q_tf_matrix_train, o_tf_matrix_list_train)\n",
    "accuracy_train_bigrams = calculate_accuracy(selected_answers_train_bigrams, train_df['correct_index'].tolist())\n",
    "\n",
    "selected_answers_valid_bigrams = select_best_answer(q_tf_matrix_valid, o_tf_matrix_list_valid)\n",
    "accuracy_valid_bigrams = calculate_accuracy(selected_answers_valid_bigrams, valid_df['correct_index'].tolist())\n",
    "\n",
    "print(f\"Training Accuracy with Bigrams: {accuracy_train_bigrams}\")\n",
    "print(f\"Validation Accuracy with Bigrams: {accuracy_valid_bigrams}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbSqz_pgyzIe"
   },
   "source": [
    "# **Section 4: Cosine similarity of vectors from bert-base-uncased**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NxPhj6Ga-oy"
   },
   "source": [
    "## **Step 1: Load BERT Model and Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28KeRZ9fa5F8"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode texts to BERT embeddings\n",
    "def encode_with_bert(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return model_output.pooler_output  # Use the pooled output for a summary representation\n",
    "\n",
    "# Function to calculate cosine similarity between a question and its options\n",
    "def calculate_cosine_similarity(question_embedding, options_embeddings):\n",
    "    similarities = cosine_similarity(question_embedding, options_embeddings)\n",
    "    return np.argmax(similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FdW5kAIbEZH"
   },
   "source": [
    "## **Step 2: Getting Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0Ztzvh4bHGc",
    "outputId": "a76b7baf-0e85-418a-b695-93af6ab9616e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.14709851551956815\n",
      "Validation Accuracy: 0.14563106796116504\n"
     ]
    }
   ],
   "source": [
    "def select_best_answer(question_embeddings, options_embeddings_list):\n",
    "    selected_answers = []\n",
    "    for q_embedding, o_embeddings in zip(question_embeddings, options_embeddings_list):\n",
    "        # Extract the actual embedding from the wrapping list\n",
    "        q_embedding = np.array(q_embedding[0])\n",
    "        o_embeddings_matrix = np.vstack([opt[0] for opt in o_embeddings])\n",
    "\n",
    "        cos_sim = cosine_similarity([q_embedding], o_embeddings_matrix)[0]\n",
    "        selected_answers.append(np.argmax(cos_sim))\n",
    "    return selected_answers\n",
    "\n",
    "correct_predictions_train = 0\n",
    "correct_predictions_valid = 0\n",
    "\n",
    "for dataset, is_train in [(train_df, True), (valid_df, False)]:\n",
    "    for _, item in dataset.iterrows():\n",
    "        question = item['question']\n",
    "        options = item['options']\n",
    "        correct_index = item['correct_index']\n",
    "        embeddings = encode_with_bert([question] + options)\n",
    "        question_embedding = embeddings[0].reshape(1, -1)\n",
    "        options_embeddings = embeddings[1:]\n",
    "\n",
    "        selected_option_index = calculate_cosine_similarity(question_embedding, options_embeddings)\n",
    "\n",
    "        if selected_option_index == correct_index:\n",
    "            if is_train:\n",
    "                correct_predictions_train += 1\n",
    "            else:\n",
    "                correct_predictions_valid += 1\n",
    "\n",
    "# After the loop, calculate and print the accuracy\n",
    "accuracy_train = correct_predictions_train / len(train_df)\n",
    "accuracy_valid = correct_predictions_valid / len(valid_df)\n",
    "\n",
    "print(f\"Training Accuracy: {accuracy_train}\")\n",
    "print(f\"Validation Accuracy: {accuracy_valid}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9dh4I7vy33-"
   },
   "source": [
    "# **Section 5: Fine-tuning a transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "_erj993_38K0",
    "outputId": "cf270f5c-99dc-4904-84a6-aa6186426d32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1484' max='1484' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1484/1484 4:01:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7888349514563107\n",
      "Precision: 0.5784313725490197\n",
      "Recall: 0.5728155339805825\n",
      "F1 Score: 0.575609756097561\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TrainingArguments\n",
    "# Define a function to create question-option pairs\n",
    "def create_question_option_pairs(data):\n",
    "    pairs = []\n",
    "    for idx, row in data.iterrows():\n",
    "        question = row['question']\n",
    "        correct_option = row['options'][row['correct_index']]\n",
    "        options = row['options']\n",
    "        for option in options:\n",
    "            label = 1 if option == correct_option else 0\n",
    "            pair = f\"{question} [SEP] {option}\", label\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "# Create question-option pairs for training and validation sets\n",
    "train_pairs = create_question_option_pairs(train_df)\n",
    "valid_pairs = create_question_option_pairs(valid_df)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_pairs_df = pd.DataFrame(train_pairs, columns=['text', 'label'])\n",
    "valid_pairs_df = pd.DataFrame(valid_pairs, columns=['text', 'label'])\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_pairs_df['text'].tolist(), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_pairs_df['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Create Dataset objects\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, train_pairs_df['label'].tolist())\n",
    "valid_dataset = Dataset(valid_encodings, valid_pairs_df['label'].tolist())\n",
    "\n",
    "# Define the training arguments with output_dir\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',  # Specify where to save the trained model and training logs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate on the validation set\n",
    "predictions = trainer.predict(valid_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = valid_pairs_df['label'].tolist()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rJquPiMJycaG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
